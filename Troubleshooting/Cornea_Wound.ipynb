{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "import gtfparse\n",
    "import pyensembl\n",
    "pyensembl.EnsemblRelease(release=87)\n",
    "import gspread\n",
    "from  oauth2client.service_account  import ServiceAccountCredentials\n",
    "import gspread\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:oauth2client.client:Refreshing access_token\n"
     ]
    }
   ],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "        'https://www.googleapis.com/auth/drive']\n",
    "path = '/home/zach/Documents/Untitled Folder/Cornea.json'\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "# Start with a gene list \n",
    "expression_worksheet = client.open(\"Cornea Wound Gene List\").sheet1\n",
    "expression = expression_worksheet.get_all_records()\n",
    "expression_df = pd.DataFrame(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '11MMvEY4s8oUIGf5kyoA4QOKs-Br2myyNooNmSO7-nEU',\n",
       " 'updatedCells': 188,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedRange': 'Sheet1!G2:G189',\n",
       " 'updatedRows': 188}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing trnascript versions from worksheet\n",
    "# Select a range\n",
    "Transcript_ID = expression_worksheet.range('G2:G189')\n",
    "# Edit range\n",
    "for cell in Transcript_ID:\n",
    "    cell.value = cell.value.split('.')[0]\n",
    "# Update in batch\n",
    "expression_worksheet.update_cells(Transcript_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_cells(worksheet, col):\n",
    "    \"\"\"Returns a range of cells in a `worksheet`'s column `col`.\"\"\"\n",
    "    start_cell = worksheet.get_addr_int(1, col)\n",
    "    end_cell = worksheet.get_addr_int(worksheet.row_count, col)\n",
    "\n",
    "    return worksheet.range('%s:%s' % (start_cell, end_cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of codewords in codebook - 200\n"
     ]
    }
   ],
   "source": [
    "def read_codebook(cbook_fname):\n",
    "    \"\"\"Read CSV of n-bit codewords.\"\"\"\n",
    "    cwords = []\n",
    "    with open(cbook_fname, 'r') as f:\n",
    "        column_name = f.readline().strip()\n",
    "        for l in f.readlines():\n",
    "            cwords.append(l.strip())\n",
    "    return cwords\n",
    "codewords = read_codebook('/home/zach/Documents/Untitled Folder/cbook_140MHD4_200MHD2.txt') # need to fill in\n",
    "np.random.shuffle(codewords)\n",
    "print('Number of codewords in codebook -', len(codewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout_names = ['RS0095', 'RS0109', 'RS0175', 'RS0237', 'RS0307', 'RS0332', 'RS0384', 'RS0406', \n",
    "                'RS0451', 'RS0468', 'RS0548', 'RS64.0', 'RS156.0', 'RS278.0', 'RS313.0', 'RS643.0', \n",
    "                'RS740.0', 'RS810.0']\n",
    "def write_codebook(rows, fname, readout_names, codebook_style = '148MHD4'):\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('version'+','+str(1)+'\\n')\n",
    "        f.write('codebook_name'+','+codebook_style+'\\n')\n",
    "        f.write('bit_names,'+','.join(readout_names)+'\\n')\n",
    "        f.write('name, id, barcode\\n')\n",
    "        for row in rows:\n",
    "            f.write(','.join([row[0], row[1], row[2]+'\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneSymbol_to_ensembl(gene_symbol_list, biomart_download_fname,\n",
    "                          organism='mouse', min_length=1000):\n",
    "    \"\"\"\n",
    "    Look up the ensembl of gene symbols.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    with open(biomart_download_fname, 'r') as f:\n",
    "        transcript_df = pd.read_csv(f)\n",
    "        genes = [i.value for i in gene_symbol_list if i != '']\n",
    "        transcript_df = transcript_df[transcript_df[u'Gene name'].isin(genes)]\n",
    "    for cell in gene_symbol_list:\n",
    "        gene = cell.value\n",
    "        if gene == '':\n",
    "            continue\n",
    "\n",
    "        transcripts = transcript_df[transcript_df[u'Gene name'] == gene].drop_duplicates('Transcript stable ID')\n",
    "        transcripts = transcripts[transcripts[u'Transcript type']==u'protein_coding'].sort_values('Transcript length (including UTRs and CDS)', ascending=False)\n",
    "\n",
    "        if len(transcripts)==0:\n",
    "            print('Failed finding: ', gene)\n",
    "            annotations.append((gene, None))\n",
    "        elif transcripts.iloc[0]['Transcript length (including UTRs and CDS)'] > min_length:\n",
    "            annotations.append((gene, transcripts))\n",
    "        else:\n",
    "            print('Gene too short: ', gene)\n",
    "            annotations.append((gene, transcripts))\n",
    "    return annotations\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensymbl_to_geneinfo(transcript_IDs, biomart_download_fname,\n",
    "                          organism='mouse', min_length=1000):\n",
    "    annotations = []\n",
    "    with open(biomart_download_fname, 'r') as f:\n",
    "        transcript_df = pd.read_csv(f)\n",
    "        genes = [i.value for i in transcript_IDs if i != '']\n",
    "        transcript_df = transcript_df[transcript_df[u'Gene stable ID'].isin(genes)]\n",
    "    for cell in transcript_IDs:\n",
    "        gene = cell.value\n",
    "        if gene == '':\n",
    "            continue\n",
    "        transcripts = transcript_df[transcript_df[u'Gene stable ID'] == gene].drop_duplicates('Transcript stable ID')\n",
    "        transcripts = transcripts[transcripts[u'Transcript type']==u'protein_coding'].sort_values('Transcript length (including UTRs and CDS)', ascending=False)\n",
    "        if len(transcripts)==0:\n",
    "            print('Failed finding: ', gene)\n",
    "            annotations.append((gene, None))\n",
    "        elif transcripts.iloc[0]['Transcript length (including UTRs and CDS)'] > min_length:\n",
    "            annotations.append((gene, transcripts))\n",
    "        else:\n",
    "            print('Gene too short: ', gene)\n",
    "            annotations.append((gene, transcripts))\n",
    "    return annotations\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trascriptome_append(transcript_IDs,file):\n",
    "    annotations = []\n",
    "    with open(file, 'r') as f:\n",
    "        transcriptome_df = pd.read_csv(f)\n",
    "        genes = [i.value for i in transcript_IDs if i != '']\n",
    "        transcriptome_df = transcriptome_df[transcriptome_df[u'transcript_id'].isin(genes)] \n",
    "    for cell in transcript_IDs:\n",
    "        gene = cell.value\n",
    "        if gene == '':\n",
    "            continue\n",
    "        transcripts = transcript_df[transcript_df[u'transcript_id'] == gene]\n",
    "        if len(transcripts)==0:\n",
    "            print('Failed finding: ', gene)\n",
    "            annotations.append((gene, None))\n",
    "        else:\n",
    "            annotations.append((gene, transcripts))\n",
    "    return annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed finding:  ENSMUST00000071134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '11MMvEY4s8oUIGf5kyoA4QOKs-Br2myyNooNmSO7-nEU',\n",
       " 'updatedCells': 188,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedRange': 'Sheet1!C2:C189',\n",
       " 'updatedRows': 188}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add FPKM data\n",
    "transcript_IDs = expression_worksheet.range('G2:G189')\n",
    "file = '/bigstore/GeneralStorage/Rob/merfish/MERFISH_analysis-master/mouse/Cornea/Expression/no_versions_isoforms_tracking.csv'\n",
    "Transcriptome_annotations = Trascriptome_append(transcript_IDs,file)\n",
    "FPKM = expression_worksheet.range('C2:C189')\n",
    "for idx, g in enumerate(Transcriptome_annotations):\n",
    "    info = g[1]\n",
    "    if info is None:\n",
    "        continue\n",
    "    fpkm = info['FPKM'].iloc[0]\n",
    "    FPKM[idx].value = fpkm\n",
    "expression_worksheet.update_cells(FPKM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene too short:  ENSMUSG00000046259\n",
      "Gene too short:  ENSMUSG00000048455\n",
      "Gene too short:  ENSMUSG00000056054\n",
      "Gene too short:  ENSMUSG00000044303\n",
      "Gene too short:  ENSMUSG00000049775\n",
      "Gene too short:  ENSMUSG00000001131\n",
      "Gene too short:  ENSMUSG00000056071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '11MMvEY4s8oUIGf5kyoA4QOKs-Br2myyNooNmSO7-nEU',\n",
       " 'updatedCells': 188,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedRange': 'Sheet1!A2:A189',\n",
       " 'updatedRows': 188}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add info to spreadsheet\n",
    "# WARNING - you must figure out column numbers and change bits for new gene sets\n",
    "gene_annotations = ensymbl_to_geneinfo(expression_worksheet.range('F2:F205'), '/home/zach/Documents/Untitled Folder/mouse_gene_info2.txt', organism='mouse')\n",
    "gname = expression_worksheet.range('F2:F189')\n",
    "tname = expression_worksheet.range('G2:G189')\n",
    "length = expression_worksheet.range('D2:D189')\n",
    "descript = expression_worksheet.range('B2:B189')\n",
    "gene = expression_worksheet.range('A2:A189')\n",
    "update = []\n",
    "for idx, g in enumerate(gene_annotations):\n",
    "    info = g[1]\n",
    "    if info is None:\n",
    "        continue\n",
    "    max_transcript = info['Transcript stable ID'].iloc[0]\n",
    "    max_length = info['Transcript length (including UTRs and CDS)'].iloc[0]\n",
    "    descript_val = info['Gene description'].iloc[0]\n",
    "    gene_name = info['Gene name'].iloc[0]\n",
    "    tname[idx].value = max_transcript\n",
    "    length[idx].value = str(max_length)\n",
    "    descript[idx].value = str(descript_val)\n",
    "    gene[idx].value = str(gene_name)\n",
    "expression_worksheet.update_cells(tname)\n",
    "expression_worksheet.update_cells(length)\n",
    "expression_worksheet.update_cells(descript)\n",
    "expression_worksheet.update_cells(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Codebook\n",
    "row_tuples = []\n",
    "\n",
    "for idx, row in expression_df.drop_duplicates('Transcript ID').iterrows():\n",
    "    row_tuples.append((row['Gname'], row['Transcript ID'].split('.')[0], str(row['barcodes'])))\n",
    "\n",
    "write_codebook(row_tuples, '/home/zach/Documents/Untitled Folder/Cornea.txt', readout_names)\n",
    "row_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_merfish_oligos(fname, counts_df = None,\n",
    "                         counts_df_column='FPKM', tid_column='transcript_id'):\n",
    "    \"\"\"\n",
    "    Bit hacky - should refactor and figure out how to handle missing isoform specificity info.\n",
    "    \"\"\"\n",
    "    from Bio import SeqIO\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns=['experiemnt', 'pleft', 'ro1', 'ro2', 'ro3', \n",
    "                               'pright', 'seq', 'gene', 'tid', \n",
    "                              'start', 'length', 'gc', 'tm', 'specicity'])\n",
    "    readout_dict = {}\n",
    "    oligos = SeqIO.FastaIO.SimpleFastaParser(open(fname, 'r'))\n",
    "    rows = []\n",
    "    fpkms = []\n",
    "    for header, seq in oligos:\n",
    "        fields = header.split(' ')\n",
    "        experiment = str(fields[0])\n",
    "        primer_left = str(fields[1])\n",
    "        primer_seqL = seq[:20]\n",
    "        primer_seqR = seq[-20:]\n",
    "        readout1 = str(fields[2])\n",
    "        readout_dict[readout1] = seq[20+1:20+1+20]\n",
    "        isoSpecificity = 1\n",
    "        # Order is different if Readouts are RO1/RO2 - encoding - RO3 vs RO1 - encoding - RO2/RO3\n",
    "        # Check with if statement and handle accordingly.\n",
    "        if '__' not in fields[3]:\n",
    "            readout2 = str(fields[3])\n",
    "            encoding = str(fields[4])\n",
    "            readout3 = fields[5]\n",
    "            ro2_start_idx = 41\n",
    "            ro3_start_idx = 92\n",
    "            readout_dict[readout3] = seq[20+20+20+1+30+1:20+1+20+20+30+1+20]\n",
    "            readout_dict[readout2] = seq[20+1+20:20+1+20+20]\n",
    "            primer_right = fields[6]\n",
    "            gene, tid, start, length, gc, tm, specificity  = encoding.split('__')\n",
    "\n",
    "\n",
    "#             gene, tid, start, length, gc, tm, specificity, isoSpecificity  = encoding.split('__')\n",
    "            encoding_region = seq[20+1+20+20:20+20+1+20+30]\n",
    "        else:\n",
    "            encoding = fields[3]\n",
    "#             isoSpecificity = fields[4]\n",
    "            readout2 = fields[4]\n",
    "            readout3 = fields[5]\n",
    "            ro2_start_idx = 72\n",
    "            ro3_start_idx = 92\n",
    "            readout_dict[readout2] = seq[20+2+20+30:20+20+30+20+2]\n",
    "            readout_dict[readout3] = seq[20+2+20+30+20:20+20+30+20+20+2]\n",
    "            primer_right = fields[6]\n",
    "            gene, tid, start, length, gc, tm, specificity  = encoding.split('__')\n",
    "#             gene, tid, start, length, gc, tm, specificity, isoSpecificity  = encoding.split('__')\n",
    "        # IMPLEMENT READOUT DICT In If Else\n",
    "            encoding_region = seq[20+1+20:20+1+20+30]\n",
    "        \n",
    "        rows.append([experiment, primer_seqL, readout1, readout2, readout3, \n",
    "                       primer_seqR, encoding_region, seq, gene, tid, start,\n",
    "                     length, gc, tm, specificity, isoSpecificity, header])\n",
    "    df = pd.DataFrame(rows, columns=['experiment', 'pleft', 'ro1', 'ro2', 'ro3', \n",
    "                               'pright', 'encodingRegion', 'seq', 'gene', 'tid', \n",
    "                              'start', 'length', 'gc', 'tm', 'specificity', 'isoSpecificity', 'header'])\n",
    "    df = df.drop_duplicates(subset=['gene', 'encodingRegion'])\n",
    "#     if isinstance(counts_df, pd.DataFrame):\n",
    "#         for tid in df.tid.unique():\n",
    "#             fpkm = counts_df[counts_df[tid_column]==tid][counts_df_column]\n",
    "#             tid_idx = df[df.tid==tid].index\n",
    "#             for i in tid_idx:\n",
    "#                 df.set_value(i, counts_df_column, fpkm.values[0])\n",
    "#     df = df.convert_objects(convert_numeric=True)\n",
    "#     df.sort_values(['gene', 'specificity', 'isoSpecificity'], ascending=False, inplace=True)\n",
    "#     df['iso_off_spots'] = (df[counts_df_column] - df['isoSpecificity']*df[counts_df_column])/df['isoSpecificity']\n",
    "#     df['gene_off_spots'] = (df[counts_df_column] - df['specificity']*df[counts_df_column])/df['specificity']\n",
    "#     df = df.drop_duplicates('tid')\n",
    "    return df, primer_seqL, primer_seqR, readout_dict\n",
    "\n",
    "def trim_oligos_to_fit(oligo_df, multi_transcripts_cutoff = 148, min_oligos=48):\n",
    "    df2 = oligo_df.copy()\n",
    "    c = Counter(df2.gene)\n",
    "    high_count = {}\n",
    "    for g, count in c.items():\n",
    "        if count < min_oligos:\n",
    "            print(g, count)\n",
    "#             c.pop(g)\n",
    "            if g not in ['SNAI2', 'SNAI1', 'ORAI1', 'P2RY11', 'INPP1', 'ACTA2', 'PICK1']:\n",
    "                df2.drop(df2[df2.gene==g].index, inplace=True)\n",
    "#         if count>multi_transcripts_cutoff:\n",
    "#             high_count[g] = count\n",
    "#             ixes = list(df2[df2.gene==g].index)\n",
    "#             ixes = np.random.choice(ixes, size=multi_transcripts_cutoff, replace=False)\n",
    "#             df2.drop(ixes, inplace=True)\n",
    "    return df2\n",
    "\n",
    "# def balance_readouts(df, per_tid=64, fa_out='mergos.fa'):\n",
    "#     from itertools import repeat\n",
    "#     tids = df.groupby(group)\n",
    "#     f = open(fa_out, 'w')\n",
    "#     new_df = pd.DataFrame()\n",
    "#     counters = []\n",
    "#     for name, group in tids:\n",
    "#         r_used = pd.unique(np.concatenate((group.ro1.unique(),group.ro2.unique(),group.ro3.unique())))\n",
    "        \n",
    "def balance_readouts(df, primersL, primersR, readouts, per_tid=64, group='tid',\n",
    "                     fa_out='python_mergos.fa', sep='__'):\n",
    "    verbose=False\n",
    "    from itertools import repeat\n",
    "    tids = df.groupby(group)\n",
    "    f = open(fa_out, 'w')\n",
    "    new_df = pd.DataFrame()\n",
    "    counters = {}\n",
    "    for name, group in tids:\n",
    "        counts = Counter()\n",
    "        r_used = pd.unique(np.concatenate((group.ro1.unique(),group.ro2.unique(),group.ro3.unique())))\n",
    "        r_used = np.concatenate(list(repeat(r_used, 1000)))\n",
    "        oligo_index = group.index.tolist()\n",
    "        np.random.shuffle(oligo_index)\n",
    "        oligo_index = oligo_index[:per_tid]\n",
    "        base_idx = 0\n",
    "        c = Counter()\n",
    "        for i, idx in enumerate(oligo_index):\n",
    "            ro1_seq = ''\n",
    "            ro2_seq = ''\n",
    "            ro3_seq = ''\n",
    "            oligo = ''\n",
    "            ro1=''\n",
    "            ro2=''\n",
    "            ro3=''\n",
    "#             try:\n",
    "            ro1_seq = readouts[r_used[base_idx]]\n",
    "            ro1 = r_used[base_idx]\n",
    "        \n",
    "            ro2_seq = readouts[r_used[base_idx+1]]\n",
    "            ro2 = r_used[base_idx+1]\n",
    "            \n",
    "            ro3_seq = readouts[r_used[base_idx+2]]\n",
    "            ro3 = r_used[base_idx+2]\n",
    "            \n",
    "            c.update([ro1, ro2, ro3])\n",
    "            row = group.loc[idx]\n",
    "            rand = np.random.randint(0, high=2)\n",
    "            if (ro1 not in r_used) or (ro2 not in r_used) or (ro3 not in r_used):\n",
    "                print(row)\n",
    "            if rand:\n",
    "                oligo = row.pleft+'A'+ro1_seq+ro2_seq+row.encodingRegion+'A'+ro3_seq+row.pright\n",
    "\n",
    "#                     row.set_value(idx, 'oligo', row.pleft+row.ro1+row.ro2+'A'+row.encodingRegion+'A'+row.ro3+row.pright)\n",
    "            else:\n",
    "                oligo = row.pleft+'A'+ro1_seq+row.encodingRegion+'A'+ro2_seq+ro3_seq+row.pright\n",
    "            if (len(row.encodingRegion) != 30) or (len(ro1_seq) != 20):\n",
    "\n",
    "                print(len(row.encodingRegion), len(ro1_seq))\n",
    "#                     row.set_value(idx, 'oligo', row.pleft+row.ro1+'A'+row.encodingRegion+'A'+row.ro2+row.ro3+row.pright)\n",
    "            header = \">\"+row.gene+sep+row.tid+sep+str(row.start)+sep+ro1+sep+ro2+sep+ro3+sep+row.experiment+'\\n'\n",
    "            f.write(header)\n",
    "            f.write(oligo+'\\n')\n",
    "            base_idx += 3\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 continue\n",
    "        counters[name] = c\n",
    "        if len(c.keys())>4:\n",
    "            print(name)\n",
    "    f.close()\n",
    "    return new_df, fa_out, counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '11MMvEY4s8oUIGf5kyoA4QOKs-Br2myyNooNmSO7-nEU',\n",
       " 'updatedCells': 188,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedRange': 'Sheet1!E2:E189',\n",
       " 'updatedRows': 188}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update spreadsheet with Probe design data\n",
    "# Sequences, FPKM, number of probes\n",
    "df = parse_merfish_oligos('/bigstore/GeneralStorage/Rob/merfish/MERFISH_analysis-master/mouse/Cornea_Wound/Cornea_Wound_oligos.fasta')[0]\n",
    "from collections import Counter\n",
    "counts = Counter(df.gene)\n",
    "num_oligos = expression_worksheet.range('E2:E189')\n",
    "gnames = expression_worksheet.range('A2:A189')\n",
    "for k, v in counts.items():\n",
    "    idx = [i for i, g in enumerate(gnames) if g.value==k][0]\n",
    "    num_oligos[idx].value = v\n",
    "expression_worksheet.update_cells(num_oligos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
